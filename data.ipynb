{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df20bf9d-db80-4101-9ba1-1c7ce24d2474",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sqlalchemy import create_engine\n",
    "import dateparser\n",
    "import numpy as np\n",
    "\n",
    "pd.set_option('display.width', 400)\n",
    "pd.set_option('display.max_columns', 11)\n",
    "\n",
    "# Credentials to connect to the database\n",
    "username = \"username\"\n",
    "password = \"DB_password\"\n",
    "hostname = \"DB_host\"\n",
    "dbname = \"DB_name\"\n",
    "\n",
    "# Process the initial data\n",
    "def first_data_handle():\n",
    "    engine = create_engine(f\"mysql+mysqlconnector://{username}:{password}@{hostname}/{dbname}\")\n",
    "    data = pd.read_sql_table(\"sales2\", engine)\n",
    "    data.product_price = round(data.product_price, 2)\n",
    "    zero_price_data = data[data.product_price == 0.0]\n",
    "    data.drop(zero_price_data.index, axis=0, inplace=True)\n",
    "    data.index = range(len(data))\n",
    "    with pd.option_context('mode.use_inf_as_na', True):\n",
    "        data = data.dropna(axis=0)\n",
    "    data.index = range(len(data))\n",
    "    data.to_sql(name=\"sales\", con=engine, if_exists=\"replace\", index=False, chunksize=1000)\n",
    "\n",
    "# Process the data and find the products that were purchased more than num_of_sales\n",
    "def products_with_sales(num_of_sales=1000):\n",
    "    engine = create_engine(f\"mysql+mysqlconnector://{username}:{password}@{hostname}/{dbname}\")\n",
    "    data = pd.read_sql_table(\"sales\", engine)\n",
    "    data[\"order_timestamp\"] = data[\"order_timestamp\"].astype(\"str\")\n",
    "    products_with_high_vol = data.groupby(\"product_id\").product_quantity.sum()\n",
    "    products_with_high_vol = products_with_high_vol[products_with_high_vol >= num_of_sales]\n",
    "    data = data.set_index(\"product_id\").join(products_with_high_vol, rsuffix=\"_total\")\n",
    "    data.dropna(axis=0, inplace=True)\n",
    "    data.drop(columns=\"product_quantity_total\", inplace=True)\n",
    "    data.sort_values(by=\"order_timestamp\", inplace=True)\n",
    "    first_date = dateparser.parse(data.iloc[0].order_timestamp)\n",
    "    last_date = dateparser.parse(data.iloc[-1].order_timestamp)\n",
    "    shift = 6 - ((last_date - first_date).days % 7)\n",
    "    data[\"week\"] = (((data[\"order_timestamp\"].apply(dateparser.parse) - first_date).dt.days + shift) // 7) + 1\n",
    "    data.reset_index(inplace=True)\n",
    "    data.to_sql(name=f\"products_{num_of_sales}_sales\", con=engine, index=False, if_exists=\"replace\", chunksize=10)\n",
    "\n",
    "# Data aggregation. Collect the data on a weekly basis\n",
    "def create_week_data():\n",
    "    engine = create_engine(f\"mysql+mysqlconnector://{username}:{password}@{hostname}/{dbname}\")\n",
    "    data = pd.read_sql_table(\"products_1000_sales\", engine)\n",
    "    data[\"order_timestamp\"] = data[\"order_timestamp\"].astype(\"str\")\n",
    "    demand_data = data.groupby([\"product_id\", \"week\"]).product_quantity.sum()\n",
    "    price_data = data.groupby([\"product_id\", \"week\"]).product_price.mean().round(2)\n",
    "    week_data = pd.concat([demand_data, price_data], axis=1).reset_index()\n",
    "    products = week_data.product_id.unique()\n",
    "    cost = pd.Series(dtype=float)\n",
    "    max_prices = pd.Series(dtype=float)\n",
    "    for product in products:\n",
    "        min_price = week_data.loc[week_data.product_id == product].product_price.min()\n",
    "        max_price = week_data.loc[week_data.product_id == product].product_price.max()\n",
    "        temp_ind = week_data.loc[week_data.product_id == product].index\n",
    "        for i in temp_ind:\n",
    "            cost.at[i] = round(0.8 * min_price, 2)\n",
    "            max_prices.at[i] = round(1.2 * max_price, 2)\n",
    "    week_data[\"product_cost\"] = cost\n",
    "    week_data[\"product_max_bound\"] = max_prices\n",
    "    week_data.to_sql(name=\"week_data\", con=engine, index=False, if_exists=\"replace\", chunksize=1000)\n",
    "\n",
    "def full_weeks(missing_weeks, total_weeks):\n",
    "    full_weeks = pd.DataFrame(columns=missing_weeks.columns)\n",
    "    for week in range(1, total_weeks+1):\n",
    "        flag = week in missing_weeks[\"week\"].values\n",
    "        temp = [\n",
    "            missing_weeks.loc[0, \"product_id\"],\n",
    "            week,\n",
    "            missing_weeks.loc[missing_weeks[\"week\"] == week, \"product_quantity\"].values[0] if flag else 0,\n",
    "            missing_weeks.loc[missing_weeks[\"week\"] == week, \"product_price\"].values[0] if flag else 0,\n",
    "            missing_weeks.loc[0, \"product_cost\"],\n",
    "            missing_weeks.loc[0, \"product_max_bound\"]\n",
    "        ]\n",
    "        full_weeks.loc[week - 1] = temp\n",
    "    return full_weeks\n",
    "\n",
    "def nn_row(row, full_weeks, number_of_weeks):\n",
    "    row_data = [\n",
    "        row[\"week\"], row[\"product_cost\"], row[\"product_max_bound\"], row[\"product_id\"]\n",
    "    ]\n",
    "    weeks = np.zeros((number_of_weeks, 2))\n",
    "    for i in range(1, number_of_weeks+1):\n",
    "        temp_week = row[\"week\"] - i\n",
    "        if temp_week >= 1:\n",
    "            p = full_weeks.loc[full_weeks[\"week\"] == temp_week, \"product_price\"].values[0]\n",
    "            q = full_weeks.loc[full_weeks[\"week\"] == temp_week, \"product_quantity\"].values[0]\n",
    "        else:\n",
    "            p = q = 0\n",
    "        weeks[number_of_weeks - i, :] = [p, q]\n",
    "    row_data.extend(weeks.flatten())\n",
    "    row_data.extend([row[\"product_price\"], row[\"product_quantity\"]])\n",
    "    return row_data\n",
    "\n",
    "def create_nn_data(number_of_weeks=16):\n",
    "    engine = create_engine(f\"mysql+mysqlconnector://{username}:{password}@{hostname}/{dbname}\")\n",
    "    week_data = pd.read_sql_table(\"week_data\", engine)\n",
    "    total_weeks = week_data.week.max()\n",
    "    columns = [\"week\", \"product_cost\", \"product_max_bound\", \"product_id\"]\n",
    "    columns.extend(f\"P{i}\" for i in range(1, number_of_weeks+2))\n",
    "    columns.extend(f\"Q{i}\" for i in range(1, number_of_weeks+2))\n",
    "    nn_data = pd.DataFrame(columns=columns)\n",
    "    for product in week_data.product_id.unique():\n",
    "        temp_product = week_data[week_data[\"product_id\"] == product].copy()\n",
    "        temp_product.reset_index(drop=True, inplace=True)\n",
    "        full_week = full_weeks(temp_product, total_weeks)\n",
    "        temp_data = pd.DataFrame(columns=columns)\n",
    "        for _, row in temp_product.iterrows():\n",
    "            temp_data.loc[len(temp_data)] = nn_row(row, full_week, number_of_weeks)\n",
    "        nn_data = pd.concat([nn_data, temp_data], ignore_index=True)\n",
    "    nn_data.to_sql(name=\"nn_data\", con=engine, index=False, if_exists=\"replace\", chunksize=1000)\n",
    "\n",
    "def pso_data():\n",
    "    engine = create_engine(f\"mysql+mysqlconnector://{username}:{password}@{hostname}/{dbname}\")\n",
    "    nn_data = pd.read_sql_table(\"nn_data\", engine)\n",
    "    total_weeks = nn_data.week.max()\n",
    "    number_of_weeks = (nn_data.shape[1] - 6) // 2\n",
    "    data = nn_data[nn_data[\"week\"] == total_weeks].copy()\n",
    "    data.index = range(len(data))\n",
    "    for i in range(1, number_of_weeks+1):\n",
    "        data[f\"P{i}\"] = data[f\"P{i+1}\"]\n",
    "        data[f\"Q{i}\"] = data[f\"Q{i+1}\"]\n",
    "    data.drop(columns=[\"week\", f\"P{number_of_weeks+1}\", f\"Q{number_of_weeks+1}\"], inplace=True)\n",
    "    pso = data.copy()\n",
    "    pso[\"product_min_bound\"] = 0\n",
    "    opt_data = pd.read_sql_table(\"data_for_optimization\", engine)\n",
    "    for product in opt_data[\"product_id\"]:\n",
    "        temp = data[data[\"product_id\"] == product].copy()\n",
    "        arxiki = opt_data[opt_data[\"product_id\"] == product].arxikiTimi.values[0]\n",
    "        teliki = opt_data[opt_data[\"product_id\"] == product].telikiTimi.values[0]\n",
    "        percentage = 1 - (teliki / arxiki)\n",
    "        max_bound = min((1 - (percentage - 0.1)) * arxiki, arxiki)\n",
    "        min_bound = (1 - (percentage + 0.1)) * arxiki\n",
    "        temp[\"product_max_bound\"] = round(float(max_bound), 2)\n",
    "        temp[\"product_min_bound\"] = round(float(min_bound), 2)\n",
    "        pso = pd.concat([pso, temp], ignore_index=True)\n",
    "    pso.to_sql(name=\"pso_data\", con=engine, index=False, if_exists=\"replace\", chunksize=10)\n",
    "\n",
    "# Helpful function to see the details of a dataframe\n",
    "def print_details(data):\n",
    "    print(f\"Number of customers: {data.customer_id.nunique()}\")\n",
    "    print(f\"Number of orders: {data.order_id.nunique()}\")\n",
    "    print(f\"Number of products: {data.product_id.nunique()}\")\n",
    "    print(f\"First date: {data.order_timestamp.min()}\")\n",
    "    print(f\"Last date: {data.order_timestamp.max()}\")\n",
    "    print(\"Columns in data:\")\n",
    "    print(list(data.columns))\n",
    "    print(data.shape)\n",
    "    print(data.head(10))\n",
    "    print(data.tail(10))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
